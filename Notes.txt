Note: these notes are to the best of my memory
Friday Jan 22nd
  -I am cleaning up the code and putting it into functions 
  -I converted time series into the total time since an arbitray date and into categories for the years days and months
  -DId a one hot encoding of the categorical variables and combined it with the time data
  -converted the target column into numbers and combined it with the existing data. 
  -used the module sklearn to perform a naive bayes classifier on the data. 
  -A three-fold cross validation was performed and it returned around 60%
  -using the pipeline function in sklearn different feature enginerering techniques were applied
Saturday jan 23rd
  -I tried runnnig the classifier with different ways of dealing with the missing values. When I impput -1 for each nan the best 
  -score was achieved. I also tried droping all rows with nan's and all columns with nan's neither produced good scores. 
  -when i try to use the model on the test data i get an error about the number of columns not matching up. 
  -during the one hot encoding there are not the same number of levels in the columns so it produces frames with different num    of columns
  -to fix this i'll combine the test and train data and then perform the one hot encoding. 
Sunday jan 24th
-I created bar charts of the female and male split on country preference
-i created plots for the country destinations, gender, age, and time. pandas has a wrapper for matplotlibs plot function which   is handy
Saturday jan 9th
- I used the pandas datareader to examine the info they have from the world  bank. The data is split up by different indicators
-You can get data between now and 1960. I explored how complete these dates were. There are about 150 countries so I set the     limit at 50 per date. 
Sunday jan 10th
-I used the requests and lxml modules to access a website and I parsed the xml tree to see where the data I wanted was
-then i put the data into a list to use with an api.  
monday jan 11th
-found all the xls links on a webpage, retrieved one of them and used pandas import excel to put it into a dataframe
-opened an excel link from consumer finance. 
-opened a weather channel csv and used pandas to get it into a dataframe
-opened a zipfile using zipfile, there is also gzip and one other format. THe file was from the machine learning database
Thursday Jan 14th
-used the world bank indicator to get categories on the countries and spplit these up based on region
-i included nan's instead of numbers for this
-made a pandas cross tab of, which is usefull for seeing the layout of two categroical variables
-made  a histogram of the income,admin,region,lending type. 
Monday February 8th
-I want to make a submission so i've got to go through what i have and pull one together. 
-i'd like to keep work on these going. and use deep learning somehow. 
February 18th Thursday
I'm learning about the R statistics project, i'm at a seminar put on by CAS IT. 
It can be used for sentiment analysis, qualitative data and quantative data. Really good for text stuff. analyze the vegetableness and the meatiness of idfferent ingredients
R studio and r interface. linux download available. sentiment anylasis, time series.
--February 20th 2016
I am going to make a teaching example with abalone. 
I would like to make a simple one and a complicated one
so I am going to make two of them. 

I printed histograms of the numberical variables and the description statistics, i printed the comparison of categories to output values, i plotted the occurence of each category. I encoded the categorical varaible in ohe fashion. 

I'm going to use sklearn to create and test models for this data set
i tested 5 different classifier with them. I'm trying to set up a grid search with random Paramaters
I'm performing feature selection before showing the data to a classifier
i wonder if there is specific preprocessing for classifiers. 
i want to be good at preprcessing. 
--February 21st 2016
downloading education analytics from the UCI repository. 
there are 7 seperate csv's. i want to merge them somehow. 
first how complete are they? there is a large discrepency between the amount in each table, there is from 2 to 1000000. so merging all these tables would provide lots of blank space. 
--i'm going back to abalone to learn scikit learn methods because i want to understand that module completley 
trying to get corss validation with the grid search to work 
--i'm getting a value error because of the targets i think
-- i'm going to learn about R now. 


