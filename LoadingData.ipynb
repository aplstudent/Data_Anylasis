{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "#from urllib2.parse import urljoin this is a python 3 package\n",
    "from os.path import basename\n",
    "import xlrd\n",
    "import xlwt\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_datareader.wb\n",
    "import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load data from the uci machine learning repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "socket = urllib.urlopen(url)\n",
    "adults = pd.read_csv(socket,  header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data'\n",
    "socket = urllib.urlopen(url)\n",
    "abalone = pd.read_csv(socket,  header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data from a local file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.chdir('/home/aplstudent/Max_Dickinson')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loaddata():\n",
    "        data = pd.read_csv('train_users_2.csv')\n",
    "        datatest  = pd.read_csv('test_users.csv')\n",
    "        return [data , datatest]\n",
    "data , datatest = loaddata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading data from a zip file found on UCI machin learning archives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'zipfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-805c1da621ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msocket\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mframe1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#21 rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'zipfile' is not defined"
     ]
    }
   ],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00349/OULAD.zip'\n",
    "response = requests.get(url)\n",
    "socket = urllib.urlopen(url)\n",
    "z = zipfile.ZipFile(StringIO.StringIO(response.content))\n",
    "frame = pd.read_csv(z.open(z.namelist()[0]))\n",
    "frame1 = pd.read_csv(z.open(z.namelist()[1])) #21 rows\n",
    "frame2 = pd.read_csv(z.open(z.namelist()[2])) #17392 rows\n",
    "frame3 = pd.read_csv(z.open(z.namelist()[3])) #32500 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the pandas data reader to pull data from world bank, other sources available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indic = pandas_datareader.wb.get_indicators() \n",
    "al = pd.DataFrame(np.empty((0,0)))\n",
    "poverty = [] \n",
    "for topic in indic.iloc[:,5].unique():\n",
    "    if 'Poverty' in topic:\n",
    "        poverty.append(topic)\n",
    "energy = []\n",
    "for topic in indic.iloc[:,5].unique():\n",
    "    if 'Energy' in topic:\n",
    "        energy.append(topic)\n",
    "al = pd.DataFrame(np.empty((0,0)))\n",
    "for single in poverty:\n",
    "    for indi in indic[indic.topics == single].iloc[:,0]:\n",
    "        allcountries = pandas_datareader.wb.download(indicator = str(indi), country = 'ALL',start = 1950, end = 2002)  # the default is for mx, can and us for 2002-2005\n",
    "        numfilled = allcountries.describe().iloc[0,0]\n",
    "        if numfilled > 100:\n",
    "            al = pd.concat((al, allcountries), join = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#s.readlines()\n",
    "#read xml data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "addinfourl instance has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-457635659557>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msocket\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: addinfourl instance has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "#url = 'http://data.consumerfinance.gov/api/views.xml'\n",
    "#socket = urllib2.Request(url)\n",
    "#s= urllib2.urlopen(socket)\n",
    "#pd.read_html(s.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find all of the links in a webpage and load them into a data frame. --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = requests.get(\"http://www.cde.ca.gov/ds/sp/ai/\")\n",
    "\n",
    "doc = html.fromstring(response.text)\n",
    "element = doc.cssselect('a')\n",
    "xlslinks = []\n",
    "for a in element:\n",
    "    link = a.attrib['href']\n",
    "    if ('sat') in link:\n",
    "        xlslinks.append(link)\n",
    "        satlinks = []\n",
    "for link in xlslinks:\n",
    "    if 'xls' in link:\n",
    "        satlinks.append(link)\n",
    "        #copy of whats below\n",
    "        base = \"http://www.cde.ca.gov/ds/sp/ai/\"\n",
    "for href in satlinks:\n",
    "    url = urljoin(base , href)\n",
    "    socket = urllib.urlopen(url) #http.client.httpresonse\n",
    "    toparse = pd.ExcelFile(socket)\n",
    "    word = toparse.parse()\n",
    "\n",
    "empty = pd.DataFrame(np.empty((0,0)))\n",
    "for i in range(2,len(satlinks)):\n",
    "    url = urljoin(base , satlinks[i])\n",
    "    #url2 = urljoin(base , xlslinks[i])\n",
    "    socket = urllib.urlopen(url) #http.client.httpresonse\n",
    "    #socket2 = urllib.request.urlopen(url2) #http.client.httpresonse\n",
    "    toparse = pd.ExcelFile(socket)\n",
    "    #toparse2 = pd.ExcelFile(socket2)\n",
    "    df1 = toparse.parse()\n",
    "    #df2 = toparse.parse()\n",
    "    empty = pd.concat((empty,df1), join = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named urljoin",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-83f2917113a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0murljoin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named urljoin"
     ]
    }
   ],
   "source": [
    "import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urlparse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a request to an api and load the save the text files locally .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name PDFDocument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-77dcd472237e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mslate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyPdf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPdfFileReader\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/aplstudent/.local/lib/python2.7/site-packages/slate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m#along with slate.  If not, see <http://www.gnu.org/licenses/>.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mslate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/aplstudent/.local/lib/python2.7/site-packages/slate/slate.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mStringIO\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdfparser\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPDFParser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPDFDocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdfinterp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPDFResourceManager\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdfinterp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPDFPageInterpreter\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mPI\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name PDFDocument"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "#import slate the site package must be changed in a few ways to get slate to work \n",
    "import PDF\n",
    "from pyPdf import PdfFileReader\n",
    "import pyPdf\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import lxml.html\n",
    "from lxml import etree\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#base=\"C:\\\\Users\\\\maxwell\\Documents\\DHfasion\"\n",
    "#3ext=name\n",
    "#base+=ext\n",
    "#os.chdir(base)\n",
    "b='aesth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def urlToFile(url,fname):\n",
    "    \"\"\"create a new directory and place an  OCR textfileinit as a single string\"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    import os\n",
    "    import re\n",
    "    base=\"C:\\\\Users\\\\maxwell\\\\Documents\\\\DHfasion\"# this line will need to be changed on differnet machines it/s a local adress \n",
    "    slash='\\\\'\n",
    "    ext=slash+fname\n",
    "    e='1'\n",
    "    base+=ext\n",
    "    if os.path.isdir(base+e):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(base+e)\n",
    "        os.chdir(base+e)\n",
    "    response=requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    text = ''.join(list(soup.stripped_strings))\n",
    "    a=text.replace(r'\\n','')\n",
    "    a=re.sub(r'\\n','',text)\n",
    "    a=re.sub(r'\\\\','',a)\n",
    "    a=a.replace('■','')# can't decode character in position 0. \n",
    "    a=a.replace('—','')\n",
    "    a=re.sub(r'—','',a)\n",
    "    a=re.sub(r'•','',a)\n",
    "    try:\n",
    "        f = open(fname,'w')\n",
    "        f.write(a)\n",
    "        f.close()\n",
    "    except (UnicodeEncodeError,FileExistsError) as e:\n",
    "        \n",
    "        print(e)\n",
    "        f.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apitofile(url,curnum):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    response=requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    #print(soup.prettify())\n",
    "    links=[]\n",
    "    line=curnum\n",
    "    for link in soup.tr.find_all('a'):\n",
    "            links.append(link.get('href'))\n",
    "    linkss=pd.unique(links)\n",
    "    for link in linkss:\n",
    "        l=link.split('#')[0]\n",
    "        base='http://chroniclingamerica.loc.gov'\n",
    "        cleanurl=base+l+'ocr.txt'\n",
    "        print(cleanurl)\n",
    "        \n",
    "        b='aesth'\n",
    "        urlToFile(str(cleanurl),b+str(line))\n",
    "        line+=1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-c8847b25ccfe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0murlToFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://chroniclingamerica.loc.gov/lccn/sn83030214/1900-01-28/ed-1/seq-40/ocr/'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'48'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-42-83ac53237afe>\u001b[0m in \u001b[0;36murlToFile\u001b[1;34m(url, fname)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\\\'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'■'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'—'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'—'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "urlToFile('http://chroniclingamerica.loc.gov/lccn/sn83030214/1900-01-28/ed-1/seq-40/ocr/',(b+'48'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
